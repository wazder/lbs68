I developed a system that compares luggage photos and measures the similarity between the luggage in these photos as a percentage.
The photos include variations such as different angles, lighting conditions, and clothes placed on top of the luggage.
In some photos, the luggage is tilted, while in others it is upright. The system was designed with a primary focus on high accuracy,
while performance was secondary, and the outputs were optimized for readability and correctness.

In this project, SAM (Segment Anything Model) was used to isolate the luggage and remove background and irrelevant objects. SAM 
generated accurate masks of the luggage, ensuring that clothes or background noise did not affect the embeddings. The masked luggage 
images were then processed with the CLIP model, which produced semantic embeddings (vectors) representing the images. These embeddings
were robust to angle and lighting variations and allowed for similarity measurement aligned with human perception.

After generating embeddings for all photos, the system calculated similarity scores using cosine similarity and reported the results as
percentage similarity. This pipeline provided highly accurate comparisons by focusing solely on the masked luggage regions.

The project workflow consisted of:

1. Using SAM to generate masks of the luggage.
2. Feeding the masked luggage images into the CLIP model to obtain embeddings.
3. Storing the embeddings for each photo.
4. Computing cosine similarity between embeddings and reporting percentage similarity.
5. Optionally, leveraging FAISS for embedding storage and fast similarity searches across larger image collections.

->by wazder
